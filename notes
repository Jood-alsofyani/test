the History and Evolution of Cloud Computing. 

Cloud computing is an evolution of technology over time. The concept of cloud computing dates to the 1950s when large-scale mainframes with high-volume processing power became available.

The practice of time sharing (or resource pooling) evolved to make efficient use of the computing power of mainframes. Using dumb terminals, whose sole purpose was facilitating access to the mainframes, multiple users could access the same data storage layer and CPU power from any terminal. 

In the 1970s, with the release of an operating system called Virtual Machine (VM), it became possible for mainframes to have multiple virtual systems, or virtual machines, on a single physical node. The virtual machine operating system evolved from the 1950s application of shared access of a mainframe. By allowing multiple distinct computing environments to exist on the same physical hardware. Each virtual machine hosted guest operating systems that behaved like they had their own memory, CPU, and hard drives, even though these were shared resources. 

Virtualization thus became a technological driver and a massive catalyst for some of the most significant evolutions in communications and computing. Even 20 years ago, physical hardware was quite expensive. With the internet becoming more accessible, and the need to make hardware costs more viable, servers were virtualized into shared hosting environments, virtual private servers, and virtual dedicated servers, using the same functionality the virtual machine operating system provided. 

So, for example, if a company needed an ‘x’ number of physical systems to run their applications, they could split one physical node into multiple virtual systems.

 A hypervisor is a small software layer that enables multiple operating systems to run alongside each other, sharing the same physical computing resources. A hypervisor also separates the Virtual Machines logically, assigning each slice of the underlying computing power, memory, and storage, preventing the virtual machines from interfering with each other. 

 So if, for example, one operating system suffers a crash or a security compromise, the others can keep working. As technologies and hypervisors improved and could share and deliver resources reliably, some companies decided to make the cloud’s benefits accessible to users. 

 These users did not have an abundance of physical servers to create their cloud computing infrastructure. Since the servers were already online, spinning up a new instance was instantaneous. Users could now order cloud resources from a larger pool of available resources and pay for them on a per-use basis, also known as pay-as-you-go. This pay-as-you-go or utility computing model, became one of the key drivers behind cloud computing's launching. 

 The pay-per-use model allowed companies and even individual developers to pay for the computing resources as and when they used them, just like units of electricity. This allowed them to switch to a more cash-flow friendly OpEx model from a CapEx model. This model appealed to all sizes of companies, those who had little or no hardware, and even those that had lots of hardware, because now, instead of making substantial capital expenditures in hardware, they could pay for compute resources as and when needed. It also allowed them to scale their workloads during usage peaks, and scale down when usage subsided. And this gave rise to modern-day cloud computing.


 